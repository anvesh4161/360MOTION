"""
Universal Video Analysis Dashboard
No complex dependencies required - uses OpenCV and basic ML
"""

import json
import math
import tempfile
import warnings
from pathlib import Path
from typing import Dict, List, Optional
import cv2
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st
from PIL import Image

# Suppress warnings
warnings.filterwarnings('ignore')

# ---------------------------------------------------------------------
# Global Plotly style
# ---------------------------------------------------------------------
px.defaults.template = "plotly_dark"
px.defaults.color_discrete_sequence = px.colors.qualitative.Set2

# ---------------------------------------------------------------------
# Video processing utilities (Simplified - No ffmpeg required)
# ---------------------------------------------------------------------
def extract_video_metadata(video_path: Path) -> Dict:
    """Extract basic video metadata using OpenCV"""
    cap = cv2.VideoCapture(str(video_path))
    
    if not cap.isOpened():
        return {'width': 640, 'height': 480, 'fps': 30, 'duration': 0}
    
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    if fps > 0:
        duration = total_frames / fps
    else:
        duration = 0
        fps = 30  # Default assumption
    
    cap.release()
    
    return {
        'width': width,
        'height': height,
        'fps': fps,
        'duration': duration,
        'total_frames': total_frames
    }

# ---------------------------------------------------------------------
# SIMPLIFIED Motion Detection (No pose estimation required)
# ---------------------------------------------------------------------
def detect_motion_in_frame(frame, prev_frame):
    """Simplified motion detection using frame differencing"""
    if prev_frame is None:
        return 0, None
    
    # Convert to grayscale
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
    
    # Calculate frame difference
    frame_diff = cv2.absdiff(gray, prev_gray)
    
    # Apply threshold
    _, thresh = cv2.threshold(frame_diff, 25, 255, cv2.THRESH_BINARY)
    
    # Calculate motion intensity
    motion_intensity = np.sum(thresh) / (255 * thresh.size)
    
    # Find motion center (simplified COM)
    motion_pixels = np.where(thresh > 0)
    if len(motion_pixels[0]) > 0:
        com_y = np.mean(motion_pixels[0]) if len(motion_pixels[0]) > 0 else height/2
        com_x = np.mean(motion_pixels[1]) if len(motion_pixels[1]) > 0 else width/2
        motion_center = (com_x, com_y)
    else:
        motion_center = None
    
    return motion_intensity, motion_center

# ---------------------------------------------------------------------
# SIMPLIFIED Face Detection (No DeepFace required)
# ---------------------------------------------------------------------
def detect_faces_in_frame(frame):
    """Simplified face detection using OpenCV's Haar Cascade"""
    # Load pre-trained face detector
    face_cascade = cv2.CascadeClassifier(
        cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
    )
    
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(
        gray,
        scaleFactor=1.1,
        minNeighbors=5,
        minSize=(30, 30)
    )
    
    return len(faces), faces

# ---------------------------------------------------------------------
# SIMPLIFIED Emotion Simulation (No ML model required)
# ---------------------------------------------------------------------
def simulate_emotions_from_motion(motion_intensity, face_count):
    """Simulate emotions based on motion and face count"""
    # Base probabilities that vary with motion
    motion_factor = min(motion_intensity * 5, 1.0)
    people_factor = min(face_count / 10, 1.0)
    
    # Simulate emotion probabilities
    happy_prob = min(0.3 + motion_factor * 0.5, 0.9)
    neutral_prob = max(0.4 - motion_factor * 0.3, 0.1)
    sad_prob = max(0.1 - motion_factor * 0.08, 0.01)
    angry_prob = min(0.05 + motion_factor * 0.1, 0.2)
    surprise_prob = min(0.05 + motion_factor * 0.3, 0.4)
    
    # Normalize
    total = happy_prob + neutral_prob + sad_prob + angry_prob + surprise_prob
    happy_prob /= total
    neutral_prob /= total
    sad_prob /= total
    angry_prob /= total
    surprise_prob /= total
    
    # Determine dominant emotion
    emotions = {
        'happy': happy_prob,
        'neutral': neutral_prob,
        'sad': sad_prob,
        'angry': angry_prob,
        'surprise': surprise_prob
    }
    dominant = max(emotions.items(), key=lambda x: x[1])[0]
    
    # Calculate positive/negative/neutral
    positive = happy_prob + surprise_prob
    negative = sad_prob + angry_prob
    neutral = neutral_prob
    
    return {
        'face_count': face_count,
        'dominant_emotion': dominant,
        'emotion_probs': emotions,
        'positive_prob': positive,
        'negative_prob': negative,
        'neutral_prob': neutral
    }

# ---------------------------------------------------------------------
# Video analysis pipeline (Simplified)
# ---------------------------------------------------------------------
def analyze_video_simple(video_path: Path, sample_rate: int = 5, max_frames: int = 500) -> Dict:
    """Simplified video analysis pipeline"""
    
    cap = cv2.VideoCapture(str(video_path))
    if not cap.isOpened():
        st.error("Could not open video file")
        return {}
    
    fps = cap.get(cv2.CAP_PROP_FPS)
    if fps <= 0:
        fps = 30  # Default assumption
    
    frames_data = []
    frame_count = 0
    prev_frame = None
    
    # Initialize face detector
    face_cascade = cv2.CascadeClassifier(
        cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
    )
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    if total_frames <= 0:
        total_frames = 1000  # Estimate
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        frame_count += 1
        
        # Skip frames based on sample rate
        if frame_count % sample_rate != 0:
            continue
        
        # Update progress
        progress = min(frame_count / total_frames, 1.0)
        progress_bar.progress(progress)
        status_text.text(f"Processing frame {frame_count} ({(progress*100):.0f}%)")
        
        h, w, _ = frame.shape
        time_sec = frame_count / fps
        
        # Detect motion
        motion_intensity, motion_center = detect_motion_in_frame(frame, prev_frame)
        prev_frame = frame.copy()
        
        # Detect faces
        face_count, face_rects = detect_faces_in_frame(frame)
        
        # Simulate emotions based on motion and faces
        emotion_data = simulate_emotions_from_motion(motion_intensity, face_count)
        
        # Store frame data
        frame_info = {
            'frame_number': frame_count,
            'time_sec': time_sec,
            'width': w,
            'height': h,
            'motion_intensity': motion_intensity,
            'motion_center': motion_center,
            'face_count': face_count,
            'face_rects': face_rects.tolist() if len(face_rects) > 0 else [],
            'emotion_data': emotion_data
        }
        
        frames_data.append(frame_info)
        
        # Stop if we have enough frames
        if len(frames_data) >= max_frames:
            break
    
    cap.release()
    progress_bar.empty()
    status_text.empty()
    
    # Get metadata
    metadata = extract_video_metadata(video_path)
    
    result = {
        'metadata': metadata,
        'frames': frames_data,
        'analysis_method': 'simplified_motion_emotion',
        'note': 'This is a simplified analysis using motion detection and simulated emotions'
    }
    
    return result

# ---------------------------------------------------------------------
# Feature extraction from analysis
# ---------------------------------------------------------------------
def extract_features_from_simple_analysis(analysis_data: Dict) -> pd.DataFrame:
    """Convert simplified analysis to features dataframe"""
    frames = analysis_data.get('frames', [])
    
    rows = []
    prev_center = None
    prev_time = None
    
    for fr in frames:
        t = fr['time_sec']
        w = fr['width']
        h = fr['height']
        
        motion_intensity = fr['motion_intensity']
        motion_center = fr['motion_center']
        face_count = fr['face_count']
        emotion_data = fr['emotion_data']
        
        # Calculate motion center coordinates
        if motion_center:
            com_x, com_y = motion_center
            # Approximate yaw from horizontal position
            if w > 0:
                com_yaw = (com_x / w) * 2.0 * math.pi - math.pi
            else:
                com_yaw = None
        else:
            com_x = com_y = com_yaw = None
        
        # Calculate motion speed
        if motion_center and prev_center and prev_time:
            dt = t - prev_time
            if dt > 0:
                com_speed = math.hypot(com_x - prev_center[0], com_y - prev_center[1]) / dt
            else:
                com_speed = 0.0
        else:
            com_speed = 0.0
        
        if motion_center:
            prev_center = motion_center
            prev_time = t
        
        # Get emotion data
        pos_prob = emotion_data['positive_prob']
        neu_prob = emotion_data['neutral_prob']
        neg_prob = emotion_data['negative_prob']
        dominant = emotion_data['dominant_emotion']
        
        # Count emotions (simplified - based on dominant)
        emotion_counts = {
            'angry': 1 if dominant == 'angry' else 0,
            'disgust': 0,  # Not simulated
            'fear': 0,     # Not simulated
            'happy': 1 if dominant == 'happy' else 0,
            'neutral': 1 if dominant == 'neutral' else 0,
            'sad': 1 if dominant == 'sad' else 0,
            'surprise': 1 if dominant == 'surprise' else 0
        }
        
        row = {
            'time': t,
            'n_persons': face_count,  # Using face count as person count
            'com_x': com_x,
            'com_y': com_y,
            'com_yaw_rad': com_yaw,
            'hand_open_raw': motion_intensity * 100,  # Simulate hand openness
            'com_speed_raw': com_speed,
            'frame_height': h,
            'frame_width': w,
            'n_faces': face_count,
            'positive_prob': pos_prob,
            'neutral_prob': neu_prob,
            'negative_prob': neg_prob,
            'motion_intensity': motion_intensity,
        }
        
        # Add emotion counts
        for emo, count in emotion_counts.items():
            row[f'count_{emo}'] = count
        
        rows.append(row)
    
    df = pd.DataFrame(rows)
    if 'com_yaw_rad' in df.columns:
        df['com_yaw_deg'] = np.degrees(df['com_yaw_rad'])
    
    return df

# ---------------------------------------------------------------------
# Vitality calculation
# ---------------------------------------------------------------------
def _normalize(series: pd.Series) -> pd.Series:
    """Normalize a pandas series to 0-1 range"""
    s = series.astype(float)
    min_v = s.min()
    max_v = s.max()
    if pd.isna(min_v) or pd.isna(max_v) or max_v - min_v == 0:
        return pd.Series(np.zeros(len(s)), index=s.index)
    return (s - min_v) / (max_v - min_v)

def calculate_vitality_simple(
    df: pd.DataFrame,
    w_open: float,
    w_motion: float,
    w_emo: float,
    w_people: float,
) -> pd.DataFrame:
    """Calculate vitality score from simplified features"""
    out = df.copy()
    
    # Normalize features
    out['motion_norm'] = _normalize(out['motion_intensity'].fillna(0))
    out['people_norm'] = _normalize(out['n_persons'].fillna(0))
    out['positive_norm'] = _normalize(out['positive_prob'].fillna(0))
    
    # Calculate weighted vitality
    total_w = max(w_open + w_motion + w_emo + w_people, 1e-6)
    
    vitality = (
        w_open * out['motion_norm'].fillna(0) +      # Using motion for hand openness
        w_motion * out['motion_norm'].fillna(0) +    # Using motion for body movement
        w_emo * out['positive_norm'].fillna(0) +     # Positive emotions
        w_people * out['people_norm'].fillna(0)      # People count
    ) / total_w
    
    out['vitality'] = vitality.clip(0.0, 1.0)
    return out

# ---------------------------------------------------------------------
# Visualization helpers
# ---------------------------------------------------------------------
def create_3d_trajectory_plot(df: pd.DataFrame, title: str = "3D Movement Analysis"):
    """Create 3D visualization of movement over time"""
    if df.empty or 'com_x' not in df.columns:
        return None
    
    df_clean = df.dropna(subset=['com_x', 'com_y', 'time']).copy()
    if df_clean.empty:
        return None
    
    fig = go.Figure()
    
    # Add trajectory line
    fig.add_trace(go.Scatter3d(
        x=df_clean['com_x'],
        y=df_clean['time'],
        z=df_clean['com_y'],
        mode='lines',
        line=dict(color='cyan', width=4),
        name='Movement Path'
    ))
    
    # Add vitality markers
    if 'vitality' in df_clean.columns:
        fig.add_trace(go.Scatter3d(
            x=df_clean['com_x'],
            y=df_clean['time'],
            z=df_clean['com_y'],
            mode='markers',
            marker=dict(
                size=8,
                color=df_clean['vitality'],
                colorscale='Viridis',
                showscale=True,
                colorbar=dict(title="Vitality")
            ),
            name='Vitality'
        ))
    
    fig.update_layout(
        title=title,
        scene=dict(
            xaxis_title='X Position',
            yaxis_title='Time (s)',
            zaxis_title='Y Position',
            camera=dict(eye=dict(x=1.5, y=1.5, z=1.5))
        ),
        height=500
    )
    
    return fig

def create_spherical_view(df: pd.DataFrame, title: str = "Activity Distribution"):
    """Create spherical view of activity"""
    if df.empty or 'com_x' not in df.columns:
        return None
    
    df_clean = df.dropna(subset=['com_x', 'com_y']).copy()
    if df_clean.empty:
        return None
    
    # Normalize to spherical coordinates
    width = df_clean['frame_width'].iloc[0] if 'frame_width' in df_clean.columns else 1
    height = df_clean['frame_height'].iloc[0] if 'frame_height' in df_clean.columns else 1
    
    lon = (df_clean['com_x'] / width) * 2 * np.pi - np.pi
    lat = 0.5 * np.pi - (df_clean['com_y'] / height) * np.pi
    
    x = np.cos(lat) * np.cos(lon)
    y = np.cos(lat) * np.sin(lon)
    z = np.sin(lat)
    
    fig = go.Figure()
    
    # Add sphere
    u = np.linspace(0, 2 * np.pi, 30)
    v = np.linspace(0, np.pi, 30)
    x_sphere = np.outer(np.cos(u), np.sin(v))
    y_sphere = np.outer(np.sin(u), np.sin(v))
    z_sphere = np.outer(np.ones(np.size(u)), np.cos(v))
    
    fig.add_trace(go.Surface(
        x=x_sphere, y=y_sphere, z=z_sphere,
        opacity=0.2,
        colorscale='Gray',
        showscale=False,
        name='Sphere'
    ))
    
    # Add activity points
    if 'vitality' in df_clean.columns:
        fig.add_trace(go.Scatter3d(
            x=x,
            y=y,
            z=z,
            mode='markers',
            marker=dict(
                size=10,
                color=df_clean['vitality'],
                colorscale='Plasma',
                showscale=True,
                colorbar=dict(title="Vitality")
            ),
            name='Activity'
        ))
    
    fig.update_layout(
        title=title,
        scene=dict(
            xaxis_title='X',
            yaxis_title='Y',
            zaxis_title='Z',
            aspectmode='data'
        ),
        height=500
    )
    
    return fig

# ---------------------------------------------------------------------
# Main Streamlit App
# ---------------------------------------------------------------------
def main():
    st.set_page_config(
        page_title="Video Analysis Dashboard",
        layout="wide",
        page_icon="ðŸŽ¬"
    )
    
    st.title("ðŸŽ¬ Universal Video Analysis Dashboard")
    st.markdown("""
    Upload any video to analyze motion, activity, and calculate vitality scores.
    No complex dependencies required - works out of the box!
    """)
    
    # Sidebar controls
    with st.sidebar:
        st.header("ðŸ“ Video Upload")
        uploaded_file = st.file_uploader(
            "Choose a video file",
            type=['mp4', 'avi', 'mov', 'mkv', 'webm', 'flv', 'wmv', 'mpeg', 'mpg'],
            help="Upload any common video format"
        )
        
        st.markdown("---")
        st.header("âš™ï¸ Processing Settings")
        
        sample_rate = st.slider(
            "Frame sampling rate",
            min_value=1,
            max_value=30,
            value=10,
            help="Process 1 out of every N frames (higher = faster)"
        )
        
        max_frames = st.slider(
            "Maximum frames",
            min_value=50,
            max_value=1000,
            value=200,
            help="Limit total frames processed"
        )
        
        st.markdown("---")
        st.header("ðŸ“ˆ Vitality Weights")
        
        w_open = st.slider("Hand Movement", 0.0, 2.0, 1.0, 0.1)
        w_motion = st.slider("Body Movement", 0.0, 2.0, 1.0, 0.1)
        w_emo = st.slider("Positive Emotions", 0.0, 2.0, 1.0, 0.1)
        w_people = st.slider("People Count", 0.0, 2.0, 0.5, 0.1)
        
        st.markdown("---")
        st.header("ðŸ’¾ Export Options")
        export_json = st.checkbox("Export as JSON", value=True)
        export_csv = st.checkbox("Export as CSV", value=True)
    
    # Main content
    if uploaded_file is not None:
        # Save uploaded file
        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_file:
            tmp_file.write(uploaded_file.read())
            video_path = Path(tmp_file.name)
        
        # Display video info
        st.subheader("ðŸ“¹ Video Preview")
        col1, col2, col3 = st.columns(3)
        
        metadata = extract_video_metadata(video_path)
        
        with col1:
            st.metric("Duration", f"{metadata['duration']:.1f}s")
        with col2:
            st.metric("Resolution", f"{metadata['width']}Ã—{metadata['height']}")
        with col3:
            st.metric("FPS", f"{metadata['fps']:.1f}")
        
        # Show video
        st.video(str(video_path))
        
        # Process video
        if st.button("ðŸš€ Start Analysis", type="primary", use_container_width=True):
            with st.spinner("Analyzing video (this may take a moment)..."):
                # Analyze video
                analysis_result = analyze_video_simple(video_path, sample_rate, max_frames)
                
                if analysis_result:
                    # Extract features
                    df_features = extract_features_from_simple_analysis(analysis_result)
                    
                    # Calculate vitality
                    df_vitality = calculate_vitality_simple(df_features, w_open, w_motion, w_emo, w_people)
                    
                    # Store in session state
                    st.session_state['analysis_result'] = analysis_result
                    st.session_state['df_vitality'] = df_vitality
                    
                    st.success("âœ… Analysis complete!")
                else:
                    st.error("âŒ Analysis failed. Please try a different video.")
        
        # Display results if available
        if 'df_vitality' in st.session_state:
            df = st.session_state['df_vitality']
            analysis_result = st.session_state['analysis_result']
            
            # Executive summary
            st.subheader("ðŸ“Š Analysis Summary")
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                avg_vitality = df['vitality'].mean() * 100
                st.metric("Avg Vitality", f"{avg_vitality:.1f}%")
            with col2:
                peak_vitality = df['vitality'].max() * 100
                st.metric("Peak Vitality", f"{peak_vitality:.1f}%")
            with col3:
                mean_people = df['n_persons'].mean()
                st.metric("Avg People/Frame", f"{mean_people:.1f}")
            with col4:
                positive_pct = df['positive_prob'].mean() * 100
                st.metric("Positive Mood", f"{positive_pct:.1f}%")
            
            # Tabs for different views
            tab1, tab2, tab3, tab4 = st.tabs([
                "ðŸ“ˆ Vitality Analysis",
                "ðŸŽ­ Emotion & Activity",
                "ðŸŒ 3D Visualizations",
                "ðŸ’¾ Export Data"
            ])
            
            with tab1:
                st.subheader("Vitality Over Time")
                
                fig_vitality = px.line(
                    df,
                    x='time',
                    y='vitality',
                    title='Vitality Score Over Time',
                    labels={'time': 'Time (s)', 'vitality': 'Vitality (0-1)'}
                )
                fig_vitality.update_traces(line=dict(width=3, color='#EF553B'))
                st.plotly_chart(fig_vitality, use_container_width=True)
                
                col_a, col_b = st.columns(2)
                with col_a:
                    fig_hist = px.histogram(
                        df,
                        x='vitality',
                        nbins=20,
                        title='Vitality Distribution',
                        labels={'vitality': 'Vitality Score', 'count': 'Frequency'}
                    )
                    st.plotly_chart(fig_hist, use_container_width=True)
                
                with col_b:
                    fig_box = px.box(
                        df,
                        y='vitality',
                        title='Vitality Statistics',
                        labels={'vitality': 'Vitality Score'}
                    )
                    st.plotly_chart(fig_box, use_container_width=True)
            
            with tab2:
                st.subheader("Emotion Analysis")
                
                # Emotion counts
                emotion_cols = [col for col in df.columns if col.startswith('count_')]
                if emotion_cols:
                    emotion_data = []
                    for col in emotion_cols:
                        emotion = col.replace('count_', '')
                        total = df[col].sum()
                        if total > 0:
                            emotion_data.append({'Emotion': emotion, 'Count': total})
                    
                    if emotion_data:
                        df_emotions = pd.DataFrame(emotion_data)
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            fig_emotion_bar = px.bar(
                                df_emotions,
                                x='Emotion',
                                y='Count',
                                title='Emotion Distribution',
                                color='Emotion'
                            )
                            st.plotly_chart(fig_emotion_bar, use_container_width=True)
                        
                        with col2:
                            fig_emotion_pie = px.pie(
                                df_emotions,
                                values='Count',
                                names='Emotion',
                                title='Emotion Proportions',
                                hole=0.3
                            )
                            st.plotly_chart(fig_emotion_pie, use_container_width=True)
                
                # Emotion trends
                st.subheader("Emotion Trends Over Time")
                if all(col in df.columns for col in ['time', 'positive_prob', 'neutral_prob', 'negative_prob']):
                    df_melted = df[['time', 'positive_prob', 'neutral_prob', 'negative_prob']].melt(
                        id_vars=['time'],
                        var_name='emotion_type',
                        value_name='probability'
                    )
                    
                    fig_emotion_trend = px.line(
                        df_melted,
                        x='time',
                        y='probability',
                        color='emotion_type',
                        title='Emotion Probabilities Over Time',
                        labels={'time': 'Time (s)', 'probability': 'Probability'}
                    )
                    st.plotly_chart(fig_emotion_trend, use_container_width=True)
            
            with tab3:
                st.subheader("3D Visualizations")
                
                # 3D Trajectory
                fig_3d = create_3d_trajectory_plot(df, "3D Movement Analysis")
                if fig_3d:
                    st.plotly_chart(fig_3d, use_container_width=True)
                else:
                    st.info("Insufficient movement data for 3D trajectory")
                
                # Spherical view
                fig_sphere = create_spherical_view(df, "Activity Distribution")
                if fig_sphere:
                    st.plotly_chart(fig_sphere, use_container_width=True)
                else:
                    st.info("Insufficient data for spherical view")
                
                # 3D Scatter: Time vs People vs Vitality
                if all(col in df.columns for col in ['time', 'n_persons', 'vitality']):
                    fig_scatter_3d = px.scatter_3d(
                        df.dropna(subset=['time', 'n_persons', 'vitality']),
                        x='time',
                        y='n_persons',
                        z='vitality',
                        color='positive_prob',
                        title='Time vs People vs Vitality',
                        labels={
                            'time': 'Time (s)',
                            'n_persons': 'People Count',
                            'vitality': 'Vitality',
                            'positive_prob': 'Positive Emotion'
                        }
                    )
                    st.plotly_chart(fig_scatter_3d, use_container_width=True)
            
            with tab4:
                st.subheader("Export Analysis Data")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    if export_json:
                        json_data = json.dumps(analysis_result, indent=2, default=str)
                        st.download_button(
                            label="ðŸ“¥ Download JSON",
                            data=json_data,
                            file_name="video_analysis.json",
                            mime="application/json",
                            use_container_width=True
                        )
                
                with col2:
                    if export_csv:
                        csv_data = df.to_csv(index=False)
                        st.download_button(
                            label="ðŸ“¥ Download CSV",
                            data=csv_data,
                            file_name="video_analysis.csv",
                            mime="text/csv",
                            use_container_width=True
                        )
                
                st.markdown("---")
                st.subheader("Data Preview")
                
                with st.expander("View Analysis Data"):
                    st.dataframe(df.head(50))
                
                with st.expander("View Statistics"):
                    st.write(df.describe())
                
                st.markdown("---")
                st.subheader("Analysis Notes")
                st.info("""
                **Note:** This simplified analysis uses:
                - Motion detection instead of pose estimation
                - Simulated emotions based on activity level
                - Face detection for people counting
                
                For more accurate analysis, consider installing additional ML libraries.
                """)
    
    else:
        # Welcome screen
        st.info("ðŸ‘ˆ Please upload a video file to begin analysis")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            ### ðŸŽ¯ Features
            - Motion detection and analysis
            - Activity level tracking
            - Simulated emotion recognition
            - People counting
            - 2D and 3D visualizations
            - Data export (JSON/CSV)
            """)
        
        with col2:
            st.markdown("""
            ### ðŸ“‹ How to Use
            1. Upload a video (any common format)
            2. Adjust processing settings if needed
            3. Click "Start Analysis"
            4. Explore results in different tabs
            5. Export data as needed
            
            **No complex installations required!**
            """)
        
        st.markdown("---")
        st.markdown("""
        ### ðŸ”§ Technical Details
        - Built with OpenCV for video processing
        - Uses Plotly for interactive visualizations
        - No external ML models required
        - Works with most video formats
        """)

# ---------------------------------------------------------------------
# Run the app
# ---------------------------------------------------------------------
if __name__ == "__main__":
    main()